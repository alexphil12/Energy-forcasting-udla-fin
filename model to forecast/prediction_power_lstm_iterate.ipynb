{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexphil12/Energy-forcasting-UDLA/blob/main/prediction_power_lstm_iterate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1BAyr7fsvDxC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import copy as cp\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4zcZXhe7u6yE"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler,StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "AMfjxPlMBjtf"
      },
      "outputs": [],
      "source": [
        "mesure=[\"Fecha\",\"Hora\"];\n",
        "mesure.append(\"Voltaje_(R)_[V]\")\t\n",
        "mesure.append(\"Voltaje_(S)_[V]\")\t\n",
        "mesure.append(\"Voltaje_(T)_[V]\")\t\n",
        "mesure.append(\"Voltaje_(RS)_[V]\")\t\n",
        "mesure.append(\"Voltaje_(ST)_[V]\")\t\n",
        "mesure.append(\"Voltaje_(TR)_[V]\")\t\n",
        "mesure.append(\"Corriente_R_[A]\")\t\n",
        "mesure.append(\"Corriente_S_[A]\")\t\n",
        "mesure.append(\"Corriente_T_[A]\")\t\n",
        "mesure.append(\"Potencia_R_[VA]\")\t\n",
        "mesure.append(\"Potencia_S_[VA]\")\t\n",
        "mesure.append(\"Potencia_T_[VA]\")\t\n",
        "mesure.append(\"Potencia_R_[W]\")\t\n",
        "mesure.append(\"Potencia_S_[W]\")\t\n",
        "mesure.append(\"Potencia_T_[W]\")\t\n",
        "mesure.append(\"Potencia_R_[VAR]\")\t\n",
        "mesure.append(\"Potencia_S_[VAR]\")\t\n",
        "mesure.append(\"Potencia_T_[VAR]\")\t\n",
        "mesure.append(\"Corriente_N_[A]\")\t\n",
        "mesure.append(\"Frecuencia_[Hz]\")\t\n",
        "mesure.append(\"not_sure_1\")\t\n",
        "mesure.append(\"not_sure_2\")\t\n",
        "mesure.append(\"not_sure_3\")\t\n",
        "mesure.append(\"Potencia_3F_[KVA]\")\t\n",
        "mesure.append(\"Potencia_3F_[W]\")\t\n",
        "mesure.append(\"Potencia_3F_[VAR]\")\t\n",
        "mesure.append(\"Factor de Potencia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Rnf1doKA8G5W",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "27ae8047-a71a-4b7e-ebe6-c5ec66a0e833"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-0703dab66e64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/df_cov_continu_1_1.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/df_cov_continu_1_2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/df_cov_continu_1_3.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata4\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/df_cov_continu_1_4.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmesure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/df_cov_continu_1_1.txt'"
          ]
        }
      ],
      "source": [
        "data1=pd.read_csv(\"/content/df_cov_continu_1_1.txt\",sep=\",\",header=0,names=mesure)\n",
        "data2=pd.read_csv(\"/content/df_cov_continu_1_2.txt\",sep=\",\",header=0,names=mesure)\n",
        "data3=pd.read_csv(\"/content/df_cov_continu_1_3.txt\",sep=\",\",header=0,names=mesure)\n",
        "data4=pd.read_csv(\"/content/df_cov_continu_1_4.txt\",sep=\",\",header=0,names=mesure)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Wr6dnEm8G1_"
      },
      "outputs": [],
      "source": [
        "train_data_1=data1.iloc[0:round(10253*0.8),:]\n",
        "\n",
        "test_data_1=data1.iloc[round(10253*0.8):10253,:]\n",
        "\n",
        "train_data_2=data1.iloc[0:round(9641*0.8),:]\n",
        "\n",
        "test_data_2=data1.iloc[round(9641*0.8):9641,:]\n",
        "\n",
        "train_data_3=data3.iloc[0:round(8358*0.8),:]\n",
        "\n",
        "test_data_3=data3.iloc[round(8358*0.8):8358,:]\n",
        "\n",
        "train_data_4=data4.iloc[0:round(5363*0.8),:]\n",
        "\n",
        "test_data_4=data4.iloc[round(5363*0.8):5363,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhtIgJ7W8GyJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(data1.corr(),cmap=\"gray\")\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aaI0pQaZ8GsE"
      },
      "outputs": [],
      "source": [
        "#train_data_1.drop(columns=[mesure[0],mesure[3],mesure[4],mesure[1],mesure[5],mesure[6],mesure[7],mesure[8],mesure[9],mesure[10],mesure[14],mesure[15],mesure[16],mesure[17],mesure[18],mesure[19],mesure[23],mesure[24],mesure[25],mesure[26]],inplace=True)\n",
        "#test_data_1.drop(columns=[mesure[0],mesure[3],mesure[4],mesure[1],mesure[5],mesure[6],mesure[7],mesure[8],mesure[9],mesure[10],mesure[14],mesure[15],mesure[16],mesure[17],mesure[18],mesure[19],mesure[23],mesure[24],mesure[25],mesure[26]],inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zgWyn8Ju9g2"
      },
      "outputs": [],
      "source": [
        "RSG_train_1 = train_data_1.iloc[:,2].values\n",
        "RSG_test_1=test_data_1.iloc[:,2].values\n",
        "RSG_train_2=train_data_2.iloc[:,2].values\n",
        "RSG_test_2=train_data_2.iloc[:,2].values\n",
        "RSG_train_3 = train_data_3.iloc[:,2].values\n",
        "RSG_test_3=test_data_3.iloc[:,2].values\n",
        "RSG_train_4=train_data_4.iloc[:,2].values\n",
        "RSG_test_4=train_data_4.iloc[:,2].values\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.imshow(train_data_1.corr(),cmap=\"gray\")\n",
        "plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwQ0E2y8O-LP"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler2=MinMaxScaler()\n",
        "scaler3=MinMaxScaler()\n",
        "scaler4=MinMaxScaler()\n",
        "RSG_train_1=np.reshape(RSG_train_1,(-1,1))\n",
        "RSG_test_1=np.reshape(RSG_test_1,(-1,1))\n",
        "RSG_train_2=np.reshape(RSG_train_2,(-1,1))\n",
        "RSG_test_2=np.reshape(RSG_test_2,(-1,1))\n",
        "RSG_train_3=np.reshape(RSG_train_3,(-1,1))\n",
        "RSG_test_3=np.reshape(RSG_test_3,(-1,1))\n",
        "RSG_train_4=np.reshape(RSG_train_4,(-1,1))\n",
        "RSG_test_4=np.reshape(RSG_test_4,(-1,1))\n",
        "scaler.fit(RSG_train_1)\n",
        "scaler2.fit(RSG_train_2)\n",
        "scaler3.fit(RSG_train_3)\n",
        "scaler4.fit(RSG_train_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnkm2G3dO-Gm"
      },
      "outputs": [],
      "source": [
        "scaled_1_train = scaler.transform(RSG_train_1)\n",
        "scaled_1_test = scaler.transform(RSG_test_1)\n",
        "scaled_2_train=scaler2.transform(RSG_train_2)\n",
        "scaled_2_test=scaler2.transform(RSG_test_2)\n",
        "scaled_3_train = scaler.transform(RSG_train_3)\n",
        "scaled_3_test = scaler.transform(RSG_test_3)\n",
        "scaled_4_train=scaler2.transform(RSG_train_4)\n",
        "scaled_4_test=scaler2.transform(RSG_test_4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omMrkHkvS4Dv"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.sequence import TimeseriesGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1wGcVmaS4Bt"
      },
      "outputs": [],
      "source": [
        "n_input = 20\n",
        "n_features = 0\n",
        "train_generator_1 = TimeseriesGenerator(scaled_1_train, scaled_1_train, length=n_input, batch_size=10)\n",
        "test_generator_1=TimeseriesGenerator(scaled_1_test, scaled_1_test, length=n_input, batch_size=10)\n",
        "train_generator_2 = TimeseriesGenerator(scaled_2_train, scaled_2_train, length=n_input, batch_size=10)\n",
        "test_generator_2=TimeseriesGenerator(scaled_2_test, scaled_2_test, length=n_input, batch_size=10)\n",
        "train_generator_3 = TimeseriesGenerator(scaled_3_train, scaled_3_train, length=n_input, batch_size=10)\n",
        "test_generator_3=TimeseriesGenerator(scaled_3_test, scaled_3_test, length=n_input, batch_size=10)\n",
        "train_generator_4 = TimeseriesGenerator(scaled_4_train, scaled_4_train, length=n_input, batch_size=10)\n",
        "test_generator_4=TimeseriesGenerator(scaled_4_test, scaled_4_test, length=n_input, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpsxJivfS3-o"
      },
      "outputs": [],
      "source": [
        "X,y = train_generator_1[0]\n",
        "X1,y1=test_generator_2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMnJqpZ2S37r"
      },
      "outputs": [],
      "source": [
        "print(f'Predicción: \\n {y}')\n",
        "print(f'data: \\n {X}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKKoNxEnS3zq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmhnmkHDO99T"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import GRU\n",
        "from keras.layers import Dropout\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KMAjho9SBpJ"
      },
      "outputs": [],
      "source": [
        "from keras.layers.recurrent_v2 import LSTM\n",
        "# define model\n",
        "model = Sequential()\n",
        "model.add(LSTM(30, input_shape=(n_input,n_features+1),activation='relu',return_sequences=False))\n",
        "model.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model.compile(loss='mse', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5-aka9MezUl"
      },
      "outputs": [],
      "source": [
        "model1 = Sequential()\n",
        "model1.add(LSTM(30, input_shape=(n_input,n_features+1),activation='relu',return_sequences=False))\n",
        "model1.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model1.compile(loss='mse', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(LSTM(30, input_shape=(n_input,n_features+1),activation='relu',return_sequences=False))\n",
        "model2.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model2.compile(loss='mse', optimizer='adam')"
      ],
      "metadata": {
        "id": "vAwNNkTEC1lp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3 = Sequential()\n",
        "model3.add(LSTM(30, input_shape=(n_input,n_features+1),activation='relu',return_sequences=False))\n",
        "model3.add(Dense(1, activation = \"linear\"))\n",
        "\n",
        "model3.compile(loss='mse', optimizer='adam')"
      ],
      "metadata": {
        "id": "MVdmtZy9C1ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcUxCpIOSdr5"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hxiq93-dUZvY"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator_1,steps_per_epoch=10, epochs=10,validation_data=test_generator_1)\n",
        "model1.fit(train_generator_1,steps_per_epoch=10, epochs=10,validation_data=test_generator_1)\n",
        "model2.fit(train_generator_1,steps_per_epoch=10, epochs=10,validation_data=test_generator_1)\n",
        "model3.fit(train_generator_1,steps_per_epoch=10, epochs=10,validation_data=test_generator_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkKggwckUkjC"
      },
      "outputs": [],
      "source": [
        "loss_per_epoch = model.history.history['loss']\n",
        "loss_vall=model.history.history['val_loss']\n",
        "plt.plot(range(len(loss_per_epoch)),loss_per_epoch,label=\"train-loss\")\n",
        "plt.plot(range(len(loss_vall)),loss_vall,label=\"validation-loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wT3KQi76mvv"
      },
      "outputs": [],
      "source": [
        "model1.fit(train_generator_2,steps_per_epoch=10, epochs=10,validation_data=test_generator_2)\n",
        "model2.fit(train_generator_2,steps_per_epoch=10, epochs=10,validation_data=test_generator_2)\n",
        "model3.fit(train_generator_2,steps_per_epoch=10, epochs=10,validation_data=test_generator_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wbSKjQr42Gt"
      },
      "outputs": [],
      "source": [
        "loss_per_epoch = model1.history.history['loss']\n",
        "loss_vall=model1.history.history['val_loss']\n",
        "plt.plot(range(len(loss_per_epoch)),loss_per_epoch,label=\"train-loss\")\n",
        "plt.plot(range(len(loss_vall)),loss_vall,label=\"validation-loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(train_generator_3,steps_per_epoch=10, epochs=10,validation_data=test_generator_3)\n",
        "model3.fit(train_generator_3,steps_per_epoch=10, epochs=10,validation_data=test_generator_3)"
      ],
      "metadata": {
        "id": "kjxTFzzPAFJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_per_epoch = model2.history.history['loss']\n",
        "loss_vall=model2.history.history['val_loss']\n",
        "plt.plot(range(len(loss_per_epoch)),loss_per_epoch,label=\"train-loss\")\n",
        "plt.plot(range(len(loss_vall)),loss_vall,label=\"validation-loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "nQQNfNqiAIIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.fit(train_generator_3,steps_per_epoch=10, epochs=10,validation_data=test_generator_3)"
      ],
      "metadata": {
        "id": "4Ctnosj3AIEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_per_epoch = model3.history.history['loss']\n",
        "loss_vall=model3.history.history['val_loss']\n",
        "plt.plot(range(len(loss_per_epoch)),loss_per_epoch,label=\"train-loss\")\n",
        "plt.plot(range(len(loss_vall)),loss_vall,label=\"validation-loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "_wZL9317AH7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1bolW_ZBsc6"
      },
      "outputs": [],
      "source": [
        "test_predictions = []\n",
        "Q=100\n",
        "N=400\n",
        "first_eval_batch = scaled_1_train[Q:Q+n_input]\n",
        "current_batch = first_eval_batch.reshape((1, n_input, n_features+1))\n",
        "for i in range(N):\n",
        "    \n",
        "    # obtener la predicción ([0] es para obtener solo el número en lugar de [matriz])\n",
        "    current_pred = model.predict(current_batch)[0]\n",
        "    \n",
        "    # guardar la predicción\n",
        "    test_predictions.append(current_pred) \n",
        "    \n",
        "    # actualizar el lote para incluir ahora la predicción y soltar primer valor\n",
        "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6YpMTIsBsW7"
      },
      "outputs": [],
      "source": [
        "L_true=  list(data1[mesure[3]][n_input+1+Q:n_input+1+Q+N])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ90AKAiUka5"
      },
      "outputs": [],
      "source": [
        "\n",
        "trainPredict = model.predict(train_generator_1)\n",
        "trainPredict1= model1.predict(train_generator_1)\n",
        "trainPredict2 = model2.predict(train_generator_1)\n",
        "trainPredict3= model3.predict(train_generator_1)\n",
        "true_pre=scaler.inverse_transform(trainPredict)\n",
        "true_pre1=scaler.inverse_transform(trainPredict1)\n",
        "true_pre2=scaler.inverse_transform(trainPredict2)\n",
        "true_pre3=scaler.inverse_transform(trainPredict3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhP8_H47B0pd"
      },
      "outputs": [],
      "source": [
        "fig=plt.figure(figsize=(15,5))\n",
        "plt.suptitle('Time-Series Prediction')\n",
        "plt.plot(range(len(L_true)),true_pre[n_input+Q:n_input+Q+N],label='Prediction global model 0',c=\"green\")\n",
        "plt.plot(range(len(L_true)),true_pre1[n_input+Q:n_input+Q+N],label='Prediction global model 1',c=\"yellow\")\n",
        "plt.plot(range(len(L_true)),true_pre2[n_input+Q:n_input+Q+N],label='Prediction global model 2',c=\"blue\")\n",
        "plt.plot(range(len(L_true)),true_pre3[n_input+Q:n_input+Q+N],label='Prediction global model 3',c=\"purple\")\n",
        "plt.plot(range(len(L_true)),L_true,label='True value',c=\"red\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "WDSnXLHn1kGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_Model(n_input,activa, LSTM_neurons = 100, layers=3,n_features=0):\n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  if(layers>1):\n",
        "    model.add(LSTM(LSTM_neurons, activation=activa, input_shape=(n_input, n_features+1),return_sequences=True))\n",
        "    for i in range(layers-2):\n",
        "        model.add(LSTM(LSTM_neurons, activation=activa,return_sequences=True))\n",
        "    model.add(LSTM(LSTM_neurons, activation=activa))\n",
        "  else:\n",
        "    model.add(LSTM(LSTM_neurons, activation=activa, input_shape=(n_input, n_features+1)))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  return model"
      ],
      "metadata": {
        "id": "Muti0R0Xr1Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "aRcI5NrhrMuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1J0m1m_BrNCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "activa=[\"relu\",\"elu\",\"exponential\",\"sigmoid\",\"tanh\",\"softsign\"]\n",
        "min_RMSE=10\n",
        "for activa1 in activa:\n",
        "  for Back_samples in range(18,28,2): #hasta dos días y medio hacia el pasado\n",
        "    for number_neurons in range(50,126,25): \n",
        "      for batch_size in range(50,201,50):\n",
        "        for num_layers in range(1,4): \n",
        "          #if((Back_samples,activa,number_neurons,batch_size,num_layers) in combinaciones_anteriores):\n",
        "           # continue\n",
        "          print({\n",
        "              'Retrasos':Back_samples,\n",
        "              'Activation':activa1,\n",
        "              'Neuronas LSTM':number_neurons,\n",
        "              'Batch size':batch_size,\n",
        "              'LSTM Layers': num_layers\n",
        "          })\n",
        "          n_input = Back_samples # horas hacia atrás\n",
        "          n_output = 1\n",
        "          n_features = 0 # variables exógenas\n",
        "          generator = TimeseriesGenerator(scaled_1_train, scaled_1_train, length=n_input, batch_size=n_output)\n",
        "          model = create_Model(n_input,activa1,LSTM_neurons=number_neurons,layers=num_layers,n_features=n_features)\n",
        "          model.fit(generator,epochs=7,verbose=False,batch_size=batch_size)\n",
        "          predictions = []\n",
        "          for j in range(n_input,len(list(test_data_1.iloc[:,2]))):\n",
        "            model_input = scaled_1_test[j-n_input:j]\n",
        "            model_input = model_input.reshape((1, n_input, n_features+1))\n",
        "            predictions.append(model.predict(model_input)[0])\n",
        "\n",
        "          predictions = np.ndarray.flatten(np.array(predictions))  \n",
        "          test_predictions =  np.zeros(shape=(len(predictions), 5))\n",
        "          test_predictions[:,0] = predictions\n",
        "          true_predictions = scaler.inverse_transform(test_predictions)\n",
        "          rsg_predictions = true_predictions[:,0]\n",
        "          true_values = np.ndarray.flatten(test_data_1.iloc[n_input:][mesure[2]].values)\n",
        "\n",
        "          errorMSE = mean_squared_error(true_values,rsg_predictions)\n",
        "          errorMAE = mean_absolute_error(true_values,rsg_predictions)\n",
        "          errorRMSE = np.sqrt(errorMSE)\n",
        "          if(errorRMSE<min_RMSE):\n",
        "            print('BEST')\n",
        "            model.save('./drive/MyDrive/DatosInvestigacion/LSTM_1_RS.h5')\n",
        "            min_RMSE = errorRMSE\n",
        "\n",
        "          print('RMSE:',errorRMSE)\n",
        "          print('MAE',errorMAE)\n",
        "          print('min RMSE',min_RMSE)\n",
        "          resultado = {\n",
        "              'Retrasos':Back_samples,\n",
        "              'Activation':activa1,\n",
        "              'Neuronas LSTM':number_neurons,\n",
        "              'Batch size':batch_size,\n",
        "              'LSTM Layers': num_layers,\n",
        "              'MSE':errorMSE,\n",
        "              'MAE':errorMAE,\n",
        "              'RMSE':errorRMSE\n",
        "          }\n",
        "          with open(\"./drive/MyDrive/DatosInvestigacion/resultados_LSTM_1_RS.txt\",\"a\") as file:\n",
        "            resultado = json.dumps(resultado)\n",
        "            file.write(resultado)\n",
        "            file.write('\\n')"
      ],
      "metadata": {
        "id": "hm7vWVgLr1PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wh1UMrXkr1HT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "prediction power lstm iterate",
      "provenance": [],
      "mount_file_id": "16-P-Q7l1pc0tYXj1Vt7lHi4w2hiKxz9F",
      "authorship_tag": "ABX9TyPeWlJg/Ykk8mT6AgdKtZkh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}